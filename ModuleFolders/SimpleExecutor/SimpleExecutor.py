import os
import re
import threading

from Base.Base import Base
from ModuleFolders.LLMRequester.LLMRequester import LLMRequester
from ModuleFolders.TaskConfig.TaskConfig import TaskConfig
from ModuleFolders.TaskConfig.TaskType import TaskType
from ModuleFolders.TaskExecutor.TranslatorUtil import get_source_language_for_file
from ModuleFolders.ResponseExtractor.ResponseExtractor import ResponseExtractor
from ModuleFolders.ResponseExtractor.FormatExtractor import FormatExtractor
from ModuleFolders.ResponseChecker.ResponseChecker import ResponseChecker
from ModuleFolders.PromptBuilder.PromptBuilder import PromptBuilder
from ModuleFolders.PromptBuilder.PromptBuilderPolishing import PromptBuilderPolishing
from ModuleFolders.PromptBuilder.PromptBuilderFormat import PromptBuilderFormat
from ModuleFolders.NERProcessor.NERProcessor import NERProcessor

# ç®€æ˜“è¯·æ±‚å™¨
class SimpleExecutor(Base):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


        # è®¢é˜…æ¥å£æµ‹è¯•å¼€å§‹äº‹ä»¶
        self.subscribe(Base.EVENT.API_TEST_START, self.api_test_start)
        # è®¢é˜…æœ¯è¯­è¡¨ç¿»è¯‘å¼€å§‹äº‹ä»¶
        self.subscribe(Base.EVENT.GLOSS_TASK_START, self.glossary_translation_start)
        # è®¢é˜…è¡¨æ ¼ç¿»è¯‘ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TABLE_TRANSLATE_START, self.handle_table_translation_start)
        # è®¢é˜…è¡¨æ ¼æ¶¦è‰²ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TABLE_POLISH_START, self.handle_table_polish_start)
        # è®¢é˜…è¡¨æ ¼æ´¾èƒ½ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TABLE_FORMAT_START, self.handle_table_format_start)
        # è®¢é˜…æœ¯è¯­æå–ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TERM_EXTRACTION_START, self.handle_term_extraction_start)

    # å“åº”æ¥å£æµ‹è¯•å¼€å§‹äº‹ä»¶
    def api_test_start(self, event: int, data: dict):
        thread = threading.Thread(target = self.api_test, args = (event, data))
        thread.start()

    # æ¥å£æµ‹è¯•
    def api_test(self, event, data: dict):
        # è·å–å‚æ•°
        platform_tag = data.get("tag")
        platform_name = data.get("name")
        api_url = data.get("api_url")
        api_key = data.get("api_key")
        api_format = data.get("api_format")
        model_name = data.get("model")
        auto_complete = data.get("auto_complete")
        extra_body = data.get("extra_body",{})
        region = data.get("region")
        access_key = data.get("access_key")
        secret_key = data.get("secret_key")

        # è‡ªåŠ¨è¡¥å…¨APIåœ°å€
        if (platform_tag == "sakura" or platform_tag == "LocalLLM") and not api_url.endswith("/v1"):
            api_url += "/v1"
        elif auto_complete:
            version_suffixes = ["/v1", "/v2", "/v3", "/v4"]
            if not any(api_url.endswith(suffix) for suffix in version_suffixes):
                api_url += "/v1"

        # æµ‹è¯•ç»“æœ
        failure = []
        success = []

        # è§£æå¹¶åˆ†å‰²å¯†é’¥å­—ç¬¦ä¸²
        api_keys = re.sub(r"\s+","", api_key).split(",")

        # è½®è¯¢æ‰€æœ‰å¯†é’¥è¿›è¡Œæµ‹è¯•
        for api_key in api_keys:

            # æ„å»º Prompt
            messages = [
                {
                    "role": "user",
                    "content": "å°å¯çˆ±ï¼Œä½ åœ¨å¹²å˜›"
                }
            ]
            system_prompt = "ä½ æ¥ä¸‹æ¥è¦æ‰®æ¼”æˆ‘çš„å¥³æœ‹å‹ï¼Œåå­—å«æ¬£é›¨ï¼Œè¯·ä½ ä»¥å¥³æœ‹å‹çš„æ–¹å¼å›å¤æˆ‘ã€‚"

            # æ‰“å°æ—¥å¿—
            self.print("")
            self.info("æ­£åœ¨è¿›è¡Œæ¥å£æµ‹è¯• ...")
            self.info(f"æ¥å£åç§° - {platform_name}")
            self.info(f"æ¥å£åœ°å€ - {api_url}")
            self.info(f"æ¥å£å¯†é’¥ - {'*'*(len(api_key)-8)}{api_key[-8:]}") # éšè—æ•æ„Ÿä¿¡æ¯
            self.info(f"æ¨¡å‹åç§° - {model_name}")
            if extra_body:
                self.info(f"é¢å¤–å‚æ•° - {extra_body}")
            self.print(f"ç³»ç»Ÿæç¤ºè¯ - {system_prompt}")
            self.print(f"ä¿¡æ¯å†…å®¹ - {messages}")

            # æ„å»ºé…ç½®åŒ…
            platform_config = {
                "target_platform": platform_tag,
                "api_url": api_url,
                "api_key": api_key,
                "api_format": api_format,
                "model_name": model_name,
                "region":  region,
                "access_key":  access_key,
                "secret_key": secret_key,
                "extra_body": extra_body
            }

            #å°è¯•è¯·æ±‚
            requester = LLMRequester()
            skip, response_think, response_content, prompt_tokens, completion_tokens = requester.sent_request(
                messages,
                system_prompt,
                platform_config
            )

            # æµ‹è¯•æˆåŠŸ
            if skip == False:
                self.info("æ¥å£æµ‹è¯•æˆåŠŸ ...")
                self.info(f"æ¥å£è¿”å›ä¿¡æ¯ - {response_content}")
                # å‚¨å­˜ç»“æœ
                success.append(api_key)

            # æµ‹è¯•å¤±è´¥
            else:
                self.error(f"æ¥å£æµ‹è¯•å¤±è´¥ ... ")
                # å‚¨å­˜ç»“æœ
                failure.append(api_key)

            self.print("")

        # æ‰“å°ç»“æœ
        self.print("")
        self.info(f"æ¥å£æµ‹è¯•ç»“æœï¼šå…±æµ‹è¯• {len(api_keys)} ä¸ªæ¥å£ï¼ŒæˆåŠŸ {len(success)} ä¸ªï¼Œå¤±è´¥ {len(failure)} ä¸ª ...")
        if len(failure) >0:
            self.error(f"å¤±è´¥çš„æ¥å£å¯†é’¥ - {", ".join(failure)}")
        self.print("")

        # å‘é€å®Œæˆäº‹ä»¶
        self.emit(Base.EVENT.API_TEST_DONE, {
            "failure": failure,
            "success": success,
        })


    # å“åº”æœ¯è¯­è¡¨ç¿»è¯‘å¼€å§‹äº‹ä»¶
    def glossary_translation_start(self, event: int, data: dict):
        thread = threading.Thread(target = self.glossary_translation, args = (event, data))
        thread.start()

    # æœ¯è¯­è¡¨ç¿»è¯‘
    def glossary_translation(self, event, data: dict):

        # è·å–å‚æ•°
        platform_tag = data.get("tag")
        api_url = data.get("api_url")
        api_key = data.get("api_key")
        api_format = data.get("api_format")
        model_name = data.get("model")
        auto_complete = data.get("auto_complete")
        extra_body = data.get("extra_body",{})
        region = data.get("region")
        access_key = data.get("access_key")
        secret_key = data.get("secret_key")
        target_language = data.get("target_language")

        prompt_dictionary_data = data.get("prompt_dictionary_data")
        if not prompt_dictionary_data:
            self.info("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æœ¯è¯­")
            self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                "status": "null",
                "updated_data": prompt_dictionary_data
            })
            return

        # è‡ªåŠ¨è¡¥å…¨APIåœ°å€
        if platform_tag == "sakura" and not api_url.endswith("/v1"):
            api_url += "/v1"
        elif auto_complete:
            version_suffixes = ["/v1", "/v2", "/v3", "/v4"]
            if not any(api_url.endswith(suffix) for suffix in version_suffixes):
                api_url += "/v1"


        # è§£æå¹¶åˆ†å‰²å¯†é’¥å­—ç¬¦ä¸²ï¼Œå¹¶åªå–ç¬¬ä¸€ä¸ªå¯†é’¥è¿›è¡Œæµ‹è¯•
        api_keys = re.sub(r"\s+","", api_key).split(",")
        api_key = api_keys[0]


        # è·å–æœªç¿»è¯‘æœ¯è¯­
        untranslated_items = [item for item in prompt_dictionary_data if not item.get("dst")]
        if not untranslated_items:
            self.info("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æœ¯è¯­")
            self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                "status": "null",
                "updated_data": prompt_dictionary_data
            })
            return

        # åˆ†ç»„å¤„ç†ï¼ˆæ¯ç»„æœ€å¤š50ä¸ªï¼‰
        group_size = 50
        translated_count = 0
        total_groups = (len(untranslated_items) + group_size - 1) // group_size

        # è¾“å‡ºæ•´ä½“è¿›åº¦ä¿¡æ¯
        print("")
        self.info(f" å¼€å§‹æœ¯è¯­è¡¨å¾ªç¯ç¿»è¯‘ \n"
                f"â”œ æœªç¿»è¯‘æœ¯è¯­æ€»æ•°: {len(untranslated_items)}\n"
                f"â”œ åˆ†ç»„æ•°é‡: {total_groups}\n"
                f"â”” æ¯ç»„ä¸Šé™: {group_size}æœ¯è¯­")
        print("")

        # æ„å»ºå¹³å°é…ç½®
        platform_config = {
            "target_platform": platform_tag,
            "api_url": api_url,
            "api_key": api_key,
            "api_format": api_format,
            "model_name": model_name,
            "region": region,
            "access_key": access_key,
            "secret_key": secret_key,
            "extra_body": extra_body
        }

        # åˆ†ç»„ç¿»è¯‘å¤„ç†
        for group_idx in range(total_groups):
            start_idx = group_idx * group_size
            end_idx = start_idx + group_size
            current_group = untranslated_items[start_idx:end_idx]
            
            # ç»„å¤„ç†å¼€å§‹æ—¥å¿—
            print("")
            self.info(f" æ­£åœ¨å¤„ç†ç¬¬ {group_idx+1}/{total_groups} ç»„ \n"
                    f"â”œ æœ¬ç»„æœ¯è¯­èŒƒå›´: {start_idx+1}-{min(end_idx, len(untranslated_items))}\n"
                    f"â”” å®é™…å¤„ç†æ•°é‡: {len(current_group)}æœ¯è¯­")
            print("")

            # æ„é€ ç³»ç»Ÿæç¤ºè¯
            system_prompt = (
                f"Translate the source text from the glossary into {target_language} line by line, maintaining accuracy and naturalness, and output the translation wrapped in a textarea tag:\n"
                "<textarea>\n"
                f"1.{target_language}text\n"
                "</textarea>\n"
            )

            # æ„é€ æ¶ˆæ¯å†…å®¹ï¼ŒæŒ‰è¡Œæ’åˆ—ï¼Œå¹¶æ·»åŠ åºå·
            src_terms = [f"{idx+1}.{item['src']}" for idx, item in enumerate(current_group)]
            src_terms_text = "\n".join(src_terms)
            messages = [
                {
                    "role": "user",
                    "content": src_terms_text
                }
            ]

            # è¯·æ±‚å‘é€æ—¥å¿—
            print("")
            self.info(
                    f" æ­£åœ¨å‘é€APIè¯·æ±‚...\n"
                    f"â”‚ å¹³å°ç±»å‹: {platform_tag}\n"
                    f"â”‚ æ¨¡å‹åç§°: {model_name}\n"
                    f"â”” ç›®æ ‡è¯­è¨€: {target_language}")
            print("")

            # å‘é€ç¿»è¯‘è¯·æ±‚
            requester = LLMRequester()
            skip, _, response_content, _, _ = requester.sent_request(
                messages,
                system_prompt,
                platform_config
            )

            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¿”å›å¤±è´¥ä¿¡æ¯
            if skip:
                self.error(f"ç¬¬ {group_idx+1}/{total_groups} ç»„ç¿»è¯‘å¤±è´¥")
                self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                    "status": "error",
                    "message": f"ç¬¬ {group_idx+1} ç»„ç¿»è¯‘è¯·æ±‚å¤±è´¥",
                    "updated_data": None
                })
                return

            # å¦‚æœè¯·æ±‚æˆåŠŸï¼Œè§£æç¿»è¯‘ç»“æœ
            try:
                # æå–è¯‘æ–‡ç»“æœ
                textarea_contents = re.findall(r'<textarea.*?>(.*?)</textarea>', response_content, re.DOTALL)
                last_content = textarea_contents[-1]

                # åˆ†è¡Œ
                translated_terms = last_content.strip().split("\n")
                
                # å»é™¤åºå·
                translated_terms = [re.sub(r'^\d+\.', '', term).strip() for term in translated_terms]

                # æ£€æŸ¥ç¿»è¯‘ç»“æœæ•°é‡æ˜¯å¦åŒ¹é…
                if len(translated_terms) != len(current_group):
                    raise ValueError("ç¿»è¯‘ç»“æœæ•°é‡ä¸åŒ¹é…")
                    
            except Exception as e:
                self.error(f"ç¿»è¯‘ç»“æœè§£æå¤±è´¥: {str(e)}")
                self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                    "status": "error",
                    "message": f"ç¬¬ {group_idx+1} ç»„ç»“æœè§£æå¤±è´¥",
                    "updated_data": None
                })
                return

            # æ›´æ–°ç¿»è¯‘ç»“æœ
            for idx, item in enumerate(current_group):
                item["dst"] = translated_terms[idx]
            translated_count += len(current_group)

            # è¿›åº¦æ›´æ–°æ—¥å¿—
            print("")
            self.info(
                    f"â”œ æœ¬ç»„å®Œæˆæ•°é‡: {len(current_group)}\n"
                    f"â”œ ç´¯è®¡å®Œæˆè¿›åº¦: {translated_count}/{len(untranslated_items)}\n"
                    f"â”” è¿›åº¦ç™¾åˆ†æ¯”: {translated_count/len(untranslated_items):.0%}")
            print("")

        # å…¨éƒ¨å®Œæˆ
        self.info(f" æœ¯è¯­è¡¨ç¿»è¯‘å…¨éƒ¨å®Œæˆ \n"
                f"â”œ æ€»å¤„ç†ç»„æ•°: {total_groups}\n"
                f"â”œ æ€»ç¿»è¯‘æœ¯è¯­: {translated_count}\n"
                f"â”” æœ€ç»ˆçŠ¶æ€: {'æˆåŠŸ' if translated_count == len(untranslated_items) else 'å¤±è´¥'}")
        
        # å‘é€å®Œæˆäº‹ä»¶
        self.emit(Base.EVENT.GLOSS_TASK_DONE, {
            "status": "success",
            "updated_data": prompt_dictionary_data
        })

    # å“åº”è¡¨æ ¼ç¿»è¯‘å¼€å§‹äº‹ä»¶ï¼Œå¹¶å¯åŠ¨æ–°çº¿ç¨‹
    def handle_table_translation_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_table_translation, args=(data,), daemon=True)
        thread.start()

    # è¡¨æ ¼æ–‡æœ¬çš„åˆ†æ‰¹ç¿»è¯‘
    def process_table_translation(self, data: dict):
        """å¤„ç†è¡¨æ ¼æ–‡ä»¶çš„æ‰¹é‡ç¿»è¯‘ä»»åŠ¡"""
        # è§£åŒ…ä»UIä¼ æ¥çš„æ•°æ®
        file_path = data.get("file_path")
        items_to_translate = data.get("items_to_translate")
        language_stats = data.get("language_stats")

        # å‡†å¤‡ç¿»è¯‘é…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.TRANSLATION)
        platform_config = config.get_platform_configuration("translationReq")
        file_source_lang = get_source_language_for_file(config.source_language, config.target_language, language_stats)

        # ç¿»è¯‘ä»»åŠ¡åˆ†å‰²
        MAX_LINES = 10  # æœ€å¤§è¡Œæ•°
        LOG_WIDTH = 50  # æ—¥å¿—æ¡†çš„ç»Ÿä¸€å®½åº¦
        total_items = len(items_to_translate)
        num_batches = (total_items + MAX_LINES - 1) // MAX_LINES

        self.info(f" å¼€å§‹å¤„ç†è¡¨æ ¼ç¿»è¯‘ä»»åŠ¡: {os.path.basename(file_path)}")
        self.info(f"    æ€»è®¡ {total_items} è¡Œæ–‡æœ¬, å°†åˆ†ä¸º {num_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ã€‚")

        for i in range(num_batches):
            start_index = i * MAX_LINES
            end_index = start_index + MAX_LINES
            batch_items = items_to_translate[start_index:end_index]
            
            batch_num = i + 1
            log_header = f" æ‰¹æ¬¡ {batch_num}/{num_batches} "
            
            # æ„å»º0åŸºçš„æ•°å­—åºå·åŸæ–‡è¯å…¸
            source_text_dict = {str(idx): item['source_text'] for idx, item in enumerate(batch_items)}
            # æ„å»ºè¿˜åŸç”¨ç´¢å¼•åœ°å›¾
            index_map = [item['text_index'] for item in batch_items]

            # ä¸ºå½“å‰æ‰¹æ¬¡ä»»åŠ¡æ„å»ºæç¤ºè¯å†…å®¹
            messages, system_prompt, _ = PromptBuilder.generate_prompt(
                config, source_text_dict, [], file_source_lang
            )
            
            # æ—¥å¿—
            print(f"\nâ•”{'â•' * (LOG_WIDTH-2)}")
            print(f"â•‘{log_header.center(LOG_WIDTH-2)}")
            print(f"â• {'â•' * (LOG_WIDTH-2)}")
            print(f"â”œâ”€ æ­£åœ¨å‘é€è¯·æ±‚ (å…± {len(batch_items)} è¡Œ)...")
            
            # å‘é€è¯·æ±‚
            requester = LLMRequester()
            skip, _, response_content, _, _ = requester.sent_request(
                messages, system_prompt, platform_config
            )

            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦å¤±è´¥
            if skip:
                print("â”œâ”€ è¯·æ±‚å¤±è´¥ï¼Œç½‘ç»œæˆ–APIå¯†é’¥é”™è¯¯ã€‚")
                print(f"â””â”€ âŒ è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                continue

            # æ—¥å¿—è¾“å‡º
            print("â”œâ”€ æ”¶åˆ°å›å¤ï¼Œå†…å®¹å¦‚ä¸‹:")
            for line in response_content.strip().split('\n'):
                print(f"â”‚  {line}")
            print(f"â”œ{'â”€' * (LOG_WIDTH-2)}") # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿

            # æå–å’Œæ£€æŸ¥è¿”å›å†…å®¹
            print("â”œâ”€ æ­£åœ¨è§£æå’Œæ ¡éªŒå›å¤...")
            response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
            check_result, error_content = ResponseChecker.check_polish_response_content(self, config, response_content, response_dict, source_text_dict)
            
            if not check_result:
                print(f"â”œâ”€ å†…å®¹æ ¡éªŒå¤±è´¥: {error_content}")
                print(f"â””â”€ âŒ è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                continue
            
            print(f"â”œâ”€ æˆåŠŸè§£æ {len(response_dict)} æ¡ç»“æœã€‚")

            # å°†å­—ç¬¦ä¸²åºå·çš„å­—å…¸è½¬æ¢å›åŸå§‹ text_index çš„å­—å…¸-
            restored_response_dict = {
                index_map[int(temp_idx_str)]: text
                for temp_idx_str, text in response_dict.items()
            }

            # ç§»é™¤æ–‡æœ¬ä¸­çš„æ•°å­—åºå·
            updated_items_for_ui = ResponseExtractor.remove_numbered_prefix(self, restored_response_dict)

            # å‘é€è¡¨æ ¼æ›´æ–°ä¿¡å·
            self.emit(Base.EVENT.TABLE_UPDATE, {
                "file_path": file_path,
                "target_column_index": 2,
                "updated_items": updated_items_for_ui
            })
            print(f"â””â”€ âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå·²å‘é€UIæ›´æ–°ã€‚")
            print("")

        # æ›´æ–°è½¯ä»¶çŠ¶æ€
        Base.work_status = Base.STATUS.IDLE 
        self.info(f" ğŸ³ è¡¨æ ¼ç¿»è¯‘ä»»åŠ¡å·²ç»å…¨éƒ¨å®Œæˆ")                            

    # å“åº”è¡¨æ ¼æ¶¦è‰²äº‹ä»¶
    def handle_table_polish_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_table_polish, args=(data,), daemon=True)
        thread.start()

    # è¡¨æ ¼æ–‡æœ¬çš„åˆ†æ‰¹æ¶¦è‰²
    def process_table_polish(self, data: dict):
        """å¤„ç†è¡¨æ ¼æ–‡ä»¶çš„æ‰¹é‡ç¿»è¯‘ä»»åŠ¡"""
        # è§£åŒ…ä»UIä¼ æ¥çš„æ•°æ®
        file_path = data.get("file_path")
        items_to_polish = data.get("items_to_polish")

        # å‡†å¤‡ç¿»è¯‘é…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.POLISH)
        platform_config = config.get_platform_configuration("polishingReq")
        polishing_mode_selection = config.polishing_mode_selection

        # ç¿»è¯‘ä»»åŠ¡åˆ†å‰²
        MAX_LINES = 10  # æœ€å¤§è¡Œæ•°
        LOG_WIDTH = 50  # æ—¥å¿—æ¡†çš„ç»Ÿä¸€å®½åº¦
        total_items = len(items_to_polish)
        num_batches = (total_items + MAX_LINES - 1) // MAX_LINES

        self.info(f" å¼€å§‹å¤„ç†è¡¨æ ¼æ¶¦è‰²ä»»åŠ¡: {os.path.basename(file_path)}")
        self.info(f"    æ€»è®¡ {total_items} è¡Œæ–‡æœ¬, å°†åˆ†ä¸º {num_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ã€‚")

        for i in range(num_batches):
            start_index = i * MAX_LINES
            end_index = start_index + MAX_LINES
            batch_items = items_to_polish[start_index:end_index]
            
            batch_num = i + 1
            log_header = f" æ‰¹æ¬¡ {batch_num}/{num_batches} "
            
            # æ„å»º0åŸºçš„æ•°å­—åºå·åŸæ–‡è¯å…¸
            source_text_dict = {str(idx): item['source_text'] for idx, item in enumerate(batch_items)}
            # è¯‘æ–‡è¯å…¸
            translation_text_dict = {str(idx): item['translation_text'] for idx, item in enumerate(batch_items)}
            # æ„å»ºè¿˜åŸç”¨ç´¢å¼•åœ°å›¾
            index_map = [item['text_index'] for item in batch_items]

            # ç”Ÿæˆæç¤ºè¯å†…å®¹
            messages, system_prompt, extra_log = PromptBuilderPolishing.generate_prompt(
                config,
                source_text_dict,
                translation_text_dict,
                [],
            )
            
            # æ—¥å¿—
            print(f"\nâ•”{'â•' * (LOG_WIDTH-2)}")
            print(f"â•‘{log_header.center(LOG_WIDTH-2)}")
            print(f"â• {'â•' * (LOG_WIDTH-2)}")
            print(f"â”œâ”€ æ­£åœ¨å‘é€è¯·æ±‚ (å…± {len(batch_items)} è¡Œ)...")
            
            # å‘é€è¯·æ±‚
            requester = LLMRequester()
            skip, _, response_content, _, _ = requester.sent_request(
                messages, system_prompt, platform_config
            )

            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦å¤±è´¥
            if skip:
                print("â”œâ”€ è¯·æ±‚å¤±è´¥ï¼Œç½‘ç»œæˆ–APIå¯†é’¥é”™è¯¯ã€‚")
                print(f"â””â”€ âŒ è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                continue

            # æ—¥å¿—è¾“å‡º
            print("â”œâ”€ æ”¶åˆ°å›å¤ï¼Œå†…å®¹å¦‚ä¸‹:")
            for line in response_content.strip().split('\n'):
                print(f"â”‚  {line}")
            print(f"â”œ{'â”€' * (LOG_WIDTH-2)}") # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿


            # æ ¹æ®æ¶¦è‰²æ¨¡å¼è°ƒæ•´æ–‡æœ¬å¯¹è±¡
            if polishing_mode_selection == "source_text_polish":
                text_dict = source_text_dict
            elif polishing_mode_selection == "translated_text_polish":
                text_dict = translation_text_dict

            # æå–å’Œæ£€æŸ¥è¿”å›å†…å®¹
            print("â”œâ”€ æ­£åœ¨è§£æå’Œæ ¡éªŒå›å¤...")
            response_dict = ResponseExtractor.text_extraction(self, text_dict, response_content)
            check_result, error_content = ResponseChecker.check_polish_response_content(self, config, response_content, response_dict, text_dict)
            
            if not check_result:
                print(f"â”œâ”€ å†…å®¹æ ¡éªŒå¤±è´¥: {error_content}")
                print(f"â””â”€ âŒ è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                continue
            
            print(f"â”œâ”€ æˆåŠŸè§£æ {len(response_dict)} æ¡ç»“æœã€‚")

            # å°†å­—ç¬¦ä¸²åºå·çš„å­—å…¸è½¬æ¢å›åŸå§‹ text_index çš„å­—å…¸-
            restored_response_dict = {
                index_map[int(temp_idx_str)]: text
                for temp_idx_str, text in response_dict.items()
            }

            # ç§»é™¤æ–‡æœ¬ä¸­çš„æ•°å­—åºå·
            updated_items_for_ui = ResponseExtractor.remove_numbered_prefix(self, restored_response_dict)

            # å‘é€è¡¨æ ¼æ›´æ–°ä¿¡å·
            self.emit(Base.EVENT.TABLE_UPDATE, {
                "file_path": file_path,
                "target_column_index": 3,
                "updated_items": updated_items_for_ui
            })
            print(f"â””â”€ âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå·²å‘é€UIæ›´æ–°ã€‚")
            print("")

        # æ›´æ–°è½¯ä»¶çŠ¶æ€
        Base.work_status = Base.STATUS.IDLE 
        self.info(f" ğŸ³ è¡¨æ ¼æ¶¦è‰²ä»»åŠ¡å·²ç»å…¨éƒ¨å®Œæˆ")         


    # å“åº”è¡¨æ ¼æ’ç‰ˆäº‹ä»¶
    def handle_table_format_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_table_format, args=(data,), daemon=True)
        thread.start()

    # è¡¨æ ¼æ–‡æœ¬çš„å…¨é‡æ’ç‰ˆ
    def process_table_format(self, data: dict):
        """å¤„ç†è¡¨æ ¼æ–‡ä»¶çš„å…¨é‡æ’ç‰ˆä»»åŠ¡"""

        # è§£åŒ…ä»UIä¼ æ¥çš„æ•°æ®
        file_path = data.get("file_path")
        items_to_format = data.get("items_to_format")
        original_selected_indices = data.get("selected_item_indices")

        if not items_to_format:
            self.warning("æ’ç‰ˆä»»åŠ¡ä¸­æ­¢ï¼šæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æœ¬ã€‚")
            Base.work_status = Base.STATUS.IDLE
            return

        # å‡†å¤‡æ’ç‰ˆé…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.FORMAT)
        platform_config = config.get_platform_configuration("formatReq")

        # æ—¥å¿—å’Œå‡†å¤‡å·¥ä½œ
        LOG_WIDTH = 60  # æ—¥å¿—æ¡†çš„ç»Ÿä¸€å®½åº¦
        total_items = len(items_to_format)
        
        self.info(f"â–¶ï¸ å¼€å§‹å¤„ç†è¡¨æ ¼æ’ç‰ˆä»»åŠ¡: {os.path.basename(file_path)}")
        self.info(f"   æ€»è®¡ {total_items} è¡Œæ–‡æœ¬å°†ä¸€æ¬¡æ€§å¤„ç†ã€‚")
        
        print(f"\nâ•”{'â•' * (LOG_WIDTH-2)}")
        print(f"â•‘{'è¡¨æ ¼æ–‡æœ¬æ’ç‰ˆ'.center(LOG_WIDTH-2)}")
        print(f"â• {'â•' * (LOG_WIDTH-2)}")

        # æ„å»ºå®Œæ•´çš„åŸæ–‡è¯å…¸ (0-based str index)
        source_text_dict = {str(idx): item['source_text'] for idx, item in enumerate(items_to_format)}

        # ç”Ÿæˆæç¤ºè¯
        messages, system_prompt, _ = PromptBuilderFormat.generate_prompt(
            config,
            source_text_dict,
        )
        
        # å‘é€å•ä¸ªã€å®Œæ•´çš„è¯·æ±‚
        print(f"â”œâ”€ æ­£åœ¨å‘é€è¯·æ±‚ (å…± {len(source_text_dict)} è¡Œ)...")
        requester = LLMRequester()
        skip, _, response_content, _, _ = requester.sent_request(
            messages, system_prompt, platform_config
        )

        # å¤„ç†è¯·æ±‚å¤±è´¥çš„æƒ…å†µ
        if skip:
            print("â”œâ”€ âŒ è¯·æ±‚å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIå¯†é’¥é”™è¯¯ã€‚")
            print(f"â””{'â•' * (LOG_WIDTH-2)}")
            self.error("è¡¨æ ¼æ’ç‰ˆè¯·æ±‚å¤±è´¥ï¼Œä»»åŠ¡å·²ä¸­æ­¢ã€‚")
            Base.work_status = Base.STATUS.IDLE  # ä»»åŠ¡å¤±è´¥ï¼Œé‡ç½®çŠ¶æ€
            return

        # æ—¥å¿—è¾“å‡º
        print("â”œâ”€ æ”¶åˆ°å›å¤ï¼Œå†…å®¹å¦‚ä¸‹:")
        for line in response_content.strip().split('\n'):
            print(f"â”‚  {line}")
        print(f"â”œ{'â”€' * (LOG_WIDTH-2)}") # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿

        # æå–å’Œæ£€æŸ¥è¿”å›å†…å®¹
        print("â”œâ”€ æ­£åœ¨è§£æå’Œæ ¡éªŒå›å¤...")
        response_dict = FormatExtractor.text_extraction(self, response_content)
        """
        response_dict: ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å†…å®¹çš„è¡Œå·ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œå€¼æ˜¯å¦ä¸€ä¸ªå­—å…¸ï¼Œ
        åŒ…å« 'text' (è¡Œæ–‡æœ¬) å’Œ 'blank_lines_after' (è¯¥è¡Œåçš„ç©ºè¡Œæ•°)ã€‚
        ä¾‹å¦‚: {'0': {'text': 'ç¬¬ä¸€è¡Œ', 'blank_lines_after': 2}, ...}
        """

        # æ ¡éªŒè§£æç»“æœ
        if not response_dict:
            print(f"â””âŒ å†…å®¹æå–å¤±è´¥ã€‚\n")
            self.error("è¡¨æ ¼æ’ç‰ˆè§£æå¤±è´¥ï¼Œä»»åŠ¡å·²ä¸­æ­¢ã€‚")
            Base.work_status = Base.STATUS.IDLE # ä»»åŠ¡å¤±è´¥ï¼Œé‡ç½®çŠ¶æ€
            return
        
        print(f"â”œâ”€ âœ… æˆåŠŸè§£æ {len(response_dict)} æ¡ç»“æœã€‚")

        # å‘é€å•æ¬¡è¡¨æ ¼æ›´æ–°ä¿¡å·
        self.emit(Base.EVENT.TABLE_FORMAT, {
            "file_path": file_path,
            "updated_items": response_dict,      
            "selected_item_indices": original_selected_indices, 
        })
        print(f"â”” ğŸš€ å·²å‘é€UIæ›´æ–°æŒ‡ä»¤ã€‚\n")

        # 11. ä»»åŠ¡å®Œæˆï¼Œæ›´æ–°å…¨å±€çŠ¶æ€
        Base.work_status = Base.STATUS.IDLE 
        self.info(f"ğŸ³ è¡¨æ ¼æ’ç‰ˆä»»åŠ¡å·²å®Œæˆï¼")

    # å“åº”æœ¯è¯­æå–äº‹ä»¶ï¼Œå¹¶å¯åŠ¨æ–°çº¿ç¨‹
    def handle_term_extraction_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_term_extraction, args=(data,), daemon=True)
        thread.start()

    # æœ¯è¯­æå–å¤„ç†æ–¹æ³•
    def process_term_extraction(self, data: dict):
        """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œæœ¯è¯­æå–çš„æ ¸å¿ƒé€»è¾‘"""
        params = data.get("params", {})
        items_data = data.get("items_data", [])

        if not items_data:
            self.warning("æœ¯è¯­æå–ä»»åŠ¡ä¸­æ­¢ï¼šæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æœ¬ã€‚")
            self.emit(Base.EVENT.TERM_EXTRACTION_DONE, {"results": []})
            return

        self.info(f"å¼€å§‹å¤„ç†æœ¯è¯­æå–ä»»åŠ¡... å‚æ•°: {params}")
        self.info(f"å…±æ”¶åˆ° {len(items_data)} æ¡å¾…å¤„ç†æ•°æ®ã€‚")

        # å®ä¾‹åŒ–ç‹¬ç«‹çš„å¤„ç†å™¨
        processor = NERProcessor()
        
        # è°ƒç”¨å¤„ç†å™¨çš„æ–¹æ³•ï¼Œä¼ å…¥æ•°æ®å’Œå‚æ•°
        results = processor.extract_terms(
            items_data=items_data,
            language=params.get("language"),
            entity_types=params.get("entity_types")
        )
        
        self.info(f"æœ¯è¯­æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} ä¸ªæœ¯è¯­ã€‚")

        # å·¥ä½œå®Œæˆåï¼Œå‘å°„å®Œæˆäº‹ä»¶å°†ç»“æœä¼ å›UIçº¿ç¨‹
        self.emit(Base.EVENT.TERM_EXTRACTION_DONE, {"results": results})