import copy
import os
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from ModuleFolders.Base.Base import Base
from ModuleFolders.Infrastructure.LLMRequester.LLMRequester import LLMRequester
from ModuleFolders.Infrastructure.TaskConfig.TaskConfig import TaskConfig
from ModuleFolders.Infrastructure.TaskConfig.TaskType import TaskType
from ModuleFolders.Service.TaskExecutor.TranslatorUtil import get_source_language_for_file
from ModuleFolders.Domain.ResponseExtractor.ResponseExtractor import ResponseExtractor
from ModuleFolders.Domain.ResponseChecker.ResponseChecker import ResponseChecker
from ModuleFolders.Domain.PromptBuilder.PromptBuilder import PromptBuilder
from ModuleFolders.Domain.PromptBuilder.PromptBuilderPolishing import PromptBuilderPolishing
from ModuleFolders.Service.NERProcessor.NERProcessor import NERProcessor

# ç®€æ˜“è¯·æ±‚å™¨
class SimpleExecutor(Base):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # è®¢é˜…æ¥å£æµ‹è¯•å¼€å§‹äº‹ä»¶
        self.subscribe(Base.EVENT.API_TEST_START, self.api_test_start)
        # è®¢é˜…æœ¯è¯­è¡¨ç¿»è¯‘å¼€å§‹äº‹ä»¶
        self.subscribe(Base.EVENT.GLOSS_TASK_START, self.glossary_translation_start)
        # è®¢é˜…è¡¨æ ¼ç¿»è¯‘ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TABLE_TRANSLATE_START, self.handle_table_translation_start)
        # è®¢é˜…è¡¨æ ¼æ¶¦è‰²ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TABLE_POLISH_START, self.handle_table_polish_start)
        # è®¢é˜…æœ¯è¯­æå–ä»»åŠ¡äº‹ä»¶
        self.subscribe(Base.EVENT.TERM_EXTRACTION_START, self.handle_term_extraction_start)
        # è®¢é˜…æœ¯è¯­æå–ç¿»è¯‘äº‹ä»¶
        self.subscribe(Base.EVENT.TERM_TRANSLATE_SAVE_START, self.handle_term_translate_save_start)

    # å“åº”æ¥å£æµ‹è¯•å¼€å§‹äº‹ä»¶
    def api_test_start(self, event: int, data: dict):
        thread = threading.Thread(target = self.api_test, args = (event, data))
        thread.start()

    # æ¥å£æµ‹è¯•
    def api_test(self, event, data: dict):
        # è·å–å‚æ•°
        platform_tag = data.get("tag")
        platform_name = data.get("name")
        api_url = data.get("api_url", "")
        api_key = data.get("api_key")
        api_format = data.get("api_format", "")
        model_name = data.get("model")
        auto_complete = data.get("auto_complete")
        extra_body = data.get("extra_body", {})
        region = data.get("region")
        access_key = data.get("access_key")
        secret_key = data.get("secret_key")

        # å¤„ç† API åœ°å€
        if api_url:
            # åŸºç¡€æ¸…æ´—
            api_url = api_url.strip().rstrip('/')

            # è£å‰ªå†—ä½™åç¼€
            redundant_suffixes = ["/chat/completions", "/completions", "/chat"]
            for suffix in redundant_suffixes:
                if api_url.endswith(suffix):
                    api_url = api_url[:-len(suffix)].rstrip('/')
                    break

            # åˆ¤æ–­æ˜¯å¦ä¸º Anthropic æ ¼å¼
            is_anthropic = (
                    platform_tag == "anthropic"
                    or api_format.lower() == "anthropic"
            )

            # ç‰ˆæœ¬å·åç¼€åˆ—è¡¨
            version_suffixes = ["/v1", "/v2", "/v3", "/v4", "/v5", "/v6"]

            # Anthropic æ ¼å¼ç‰¹æ®Šå¤„ç†ï¼šSDK ä¼šè‡ªåŠ¨æ‹¼æ¥ /v1/messagesï¼Œéœ€è¦å»æ‰ç”¨æˆ·è¾“å…¥çš„ç‰ˆæœ¬å·
            if is_anthropic and auto_complete:
                for suffix in version_suffixes:
                    if api_url.endswith(suffix):
                        api_url = api_url[:-len(suffix)].rstrip('/')
                        break
            # é Anthropic çš„è‡ªåŠ¨è¡¥å…¨ /v1 é€»è¾‘
            elif (platform_tag in ["sakura", "LocalLLM"]) or auto_complete:
                if not any(api_url.endswith(suffix) for suffix in version_suffixes):
                    api_url += "/v1"

        # æµ‹è¯•ç»“æœ
        failure = []
        success = []

        # è§£æå¹¶åˆ†å‰²å¯†é’¥å­—ç¬¦ä¸²
        api_keys = re.sub(r"\s+","", api_key).split(",")

        # è½®è¯¢æ‰€æœ‰å¯†é’¥è¿›è¡Œæµ‹è¯•
        for api_key in api_keys:

            # æ„å»º Prompt
            messages = [
                {
                    "role": "user",
                    "content": "å°å¯çˆ±ï¼Œä½ åœ¨å¹²å˜›"
                }
            ]
            system_prompt = "ä½ æ¥ä¸‹æ¥è¦æ‰®æ¼”æˆ‘çš„å¥³æœ‹å‹ï¼Œåå­—å«æ¬£é›¨ï¼Œè¯·ä½ ä»¥å¥³æœ‹å‹çš„æ–¹å¼å›å¤æˆ‘ã€‚"

            # æ‰“å°æ—¥å¿—
            self.print("")
            self.info("æ­£åœ¨è¿›è¡Œæ¥å£æµ‹è¯• ...")
            self.info(f"æ¥å£åç§° - {platform_name}")
            self.info(f"æ¥å£åœ°å€ - {api_url}")
            self.info(f"æ¥å£å¯†é’¥ - {'*'*(len(api_key)-8)}{api_key[-8:]}") # éšè—æ•æ„Ÿä¿¡æ¯
            self.info(f"æ¨¡å‹åç§° - {model_name}")
            if extra_body:
                self.info(f"é¢å¤–å‚æ•° - {extra_body}")
            self.print(f"ç³»ç»Ÿæç¤ºè¯ - {system_prompt}")
            self.print(f"ä¿¡æ¯å†…å®¹ - {messages}")

            # æ„å»ºé…ç½®åŒ…
            platform_config = {
                "target_platform": platform_tag,
                "api_url": api_url,
                "api_key": api_key,
                "api_format": api_format,
                "model_name": model_name,
                "region":  region,
                "access_key":  access_key,
                "secret_key": secret_key,
                "extra_body": extra_body,
                "think_switch": data.get("think_switch"),
                "think_depth": data.get("think_depth"),
                "thinking_level": data.get("thinking_level"),
                "temperature": data.get("temperature"),
                "top_p": data.get("top_p")
            }

            #å°è¯•è¯·æ±‚
            requester = LLMRequester()
            skip, response_think, response_content, prompt_tokens, completion_tokens = requester.sent_request(
                messages,
                system_prompt,
                platform_config
            )

            # æµ‹è¯•æˆåŠŸ
            if skip == False:
                self.info("æ¥å£æµ‹è¯•æˆåŠŸ ...")
                self.info(f"æ¥å£è¿”å›ä¿¡æ¯ - {response_content}")
                # å‚¨å­˜ç»“æœ
                success.append(api_key)

            # æµ‹è¯•å¤±è´¥
            else:
                self.error(f"æ¥å£æµ‹è¯•å¤±è´¥ ... ")
                # å‚¨å­˜ç»“æœ
                failure.append(api_key)

            self.print("")

        # æ‰“å°ç»“æœ
        self.print("")
        self.info(f"æ¥å£æµ‹è¯•ç»“æœï¼šå…±æµ‹è¯• {len(api_keys)} ä¸ªæ¥å£ï¼ŒæˆåŠŸ {len(success)} ä¸ªï¼Œå¤±è´¥ {len(failure)} ä¸ª ...")
        if len(failure) >0:
            self.error(f"å¤±è´¥çš„æ¥å£å¯†é’¥ - {", ".join(failure)}")
        self.print("")

        # å‘é€å®Œæˆäº‹ä»¶
        self.emit(Base.EVENT.API_TEST_DONE, {
            "failure": failure,
            "success": success,
        })


    # å“åº”æœ¯è¯­è¡¨ç¿»è¯‘å¼€å§‹äº‹ä»¶
    def glossary_translation_start(self, event: int, data: dict):
        thread = threading.Thread(target = self.glossary_translation, args = (event, data))
        thread.start()

    # æœ¯è¯­è¡¨ç¿»è¯‘
    def glossary_translation(self, event, data: dict):

        # è·å–è¡¨æ ¼æ•°æ®
        prompt_dictionary_data = data.get("prompt_dictionary_data")
        if not prompt_dictionary_data:
            self.info("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æœ¯è¯­")
            self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                "status": "null",
                "updated_data": prompt_dictionary_data
            })
            return

        # è·å–æœªç¿»è¯‘æœ¯è¯­
        untranslated_items = [item for item in prompt_dictionary_data if not item.get("dst")]
        if not untranslated_items:
            self.info("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æœ¯è¯­")
            self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                "status": "null",
                "updated_data": prompt_dictionary_data
            })
            return

        # å‡†å¤‡ç¿»è¯‘é…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.TRANSLATION)
        target_language = config.target_language
        max_threads = config.actual_thread_counts

        # åˆ†ç»„å¤„ç†ï¼ˆæ¯ç»„æœ€å¤š50ä¸ªï¼‰
        group_size = 50
        total_groups = (len(untranslated_items) + group_size - 1) // group_size

        # è¾“å‡ºæ•´ä½“è¿›åº¦ä¿¡æ¯
        print("")
        self.info(f" å¼€å§‹æœ¯è¯­è¡¨å¾ªç¯ç¿»è¯‘ \n"
                f"â”œ æœªç¿»è¯‘æœ¯è¯­æ€»æ•°: {len(untranslated_items)}\n"
                f"â”œ åˆ†ç»„æ•°é‡: {total_groups}\n"
                f"â”œ æ¯ç»„ä¸Šé™: {group_size}æœ¯è¯­\n"
                f"â”” å¹¶å‘çº¿ç¨‹æ•°: {max_threads}")
        print("")

        def translate_group(group_idx: int, current_group: list) -> tuple:
            """å¤„ç†å•ç»„ç¿»è¯‘ï¼ŒæˆåŠŸè¿”å› (group_idx, [(src, dst), ...])ï¼Œå¤±è´¥è¿”å› (group_idx, None)ã€‚"""
            group_num = group_idx + 1
            try:
                platform_config = config.get_platform_configuration("translationReq")
                has_info = any(item.get("info") for item in current_group)
                system_prompt = (
                    "You are a glossary translation assistant.The user will send a glossary in this format:\n"
                    "1|Original text|Description\n"
                    "2|Original text|Description\n"
                    "3|Original text|Description\n"
                    f"Referring to the 'Description', only translate the 'Original text' into {target_language}. Strictly output the translation in the following format, wrapped in a <textarea> tag:\n"
                    "<textarea>\n"
                    "1.Translated text\n"
                    "2.Translated text\n"
                    "3.Translated text\n"
                    "</textarea>\n"
                ) if has_info else (
                    f"Translate the source text from the glossary into {target_language} line by line, maintaining accuracy and naturalness, and output the translation wrapped in a textarea tag:\n"
                    "<textarea>\n"
                    f"1.{target_language} text\n"
                    "</textarea>\n"
                )
                if has_info:
                    src_terms = [f"{idx+1}|{item['src']}|{item['info']or''}" for idx, item in enumerate(current_group)]
                else:
                    src_terms = [f"{idx+1}.{item['src']}" for idx, item in enumerate(current_group)]
                src_terms_text = "\n".join(src_terms)
                messages = [{"role": "user", "content": src_terms_text}]
                requester = LLMRequester()
                skip, _, response_content, _, _ = requester.sent_request(
                    messages, system_prompt, platform_config
                )
                if skip:
                    self.error(f"ç¬¬ {group_num}/{total_groups} ç»„ç¿»è¯‘è¯·æ±‚å¤±è´¥")
                    return (group_idx, None)
                textarea_contents = re.findall(r'<textarea.*?>(.*?)</textarea>', response_content, re.DOTALL)
                last_content = textarea_contents[-1]
                translated_terms = last_content.strip().split("\n")
                translated_terms = [re.sub(r'^\d+\.', '', term).strip() for term in translated_terms]
                if len(translated_terms) != len(current_group):
                    self.error(f"ç¬¬ {group_num}/{total_groups} ç»„ç¿»è¯‘ç»“æœæ•°é‡ä¸åŒ¹é…")
                    return (group_idx, None)
                pairs = [(item["src"], translated_terms[idx]) for idx, item in enumerate(current_group)]
                return (group_idx, pairs)
            except Exception as e:
                self.error(f"ç¬¬ {group_num}/{total_groups} ç»„å¼‚å¸¸: {e}")
                return (group_idx, None)

        successful_pairs = []
        success_groups = 0
        failed_groups = 0

        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            future_to_group = {}
            for i in range(total_groups):
                start_idx = i * group_size
                end_idx = start_idx + group_size
                current_group = untranslated_items[start_idx:end_idx]
                future = executor.submit(translate_group, i, current_group)
                future_to_group[future] = i
            for future in as_completed(future_to_group):
                try:
                    group_idx, pairs = future.result()
                    if pairs is not None:
                        successful_pairs.extend(pairs)
                        success_groups += 1
                    else:
                        failed_groups += 1
                except Exception as e:
                    self.error(f"æœ¯è¯­è¡¨ç»„æ‰§è¡Œå¼‚å¸¸: {e}")
                    failed_groups += 1

        self.info(f" æ‰€æœ‰ç»„å¤„ç†å®Œæ¯•ã€‚æˆåŠŸ: {success_groups}, å¤±è´¥: {failed_groups}")

        if not successful_pairs:
            self.emit(Base.EVENT.GLOSS_TASK_DONE, {
                "status": "error",
                "message": "æ‰€æœ‰ç»„ç¿»è¯‘å‡æœªæˆåŠŸ",
                "updated_data": None
            })
            return

        # åˆå¹¶æˆåŠŸç»“æœåˆ°å®Œæ•´æ•°æ®
        src_to_dst = dict(successful_pairs)
        updated_data = copy.deepcopy(prompt_dictionary_data)
        for item in updated_data:
            if item.get("src") in src_to_dst and not item.get("dst"):
                item["dst"] = src_to_dst[item["src"]]

        if len(successful_pairs) == len(untranslated_items):
            status = "success"
        else:
            status = "partial"

        self.info(f" æœ¯è¯­è¡¨ç¿»è¯‘å®Œæˆ \n"
                f"â”œ æˆåŠŸæœ¯è¯­: {len(successful_pairs)}/{len(untranslated_items)}\n"
                f"â”” çŠ¶æ€: {status}")
        self.emit(Base.EVENT.GLOSS_TASK_DONE, {
            "status": status,
            "updated_data": updated_data,
            "success_count": len(successful_pairs),
            "total_count": len(untranslated_items)
        })

    # å“åº”è¡¨æ ¼ç¿»è¯‘å¼€å§‹äº‹ä»¶ï¼Œå¹¶å¯åŠ¨æ–°çº¿ç¨‹
    def handle_table_translation_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_table_translation, args=(data,), daemon=True)
        thread.start()

    # è¡¨æ ¼æ–‡æœ¬çš„åˆ†æ‰¹ç¿»è¯‘
    def process_table_translation(self, data: dict):
        """å¤„ç†è¡¨æ ¼æ–‡ä»¶çš„æ‰¹é‡ç¿»è¯‘ä»»åŠ¡"""
        # è§£åŒ…ä»UIä¼ æ¥çš„æ•°æ®
        file_path = data.get("file_path")
        items_to_translate = data.get("items_to_translate")
        language_stats = data.get("language_stats")

        # å‡†å¤‡ç¿»è¯‘é…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.TRANSLATION)
        max_threads = config.actual_thread_counts # è·å–å¹¶å‘çº¿ç¨‹æ•°
        
        # é¢„è®¡ç®—æºè¯­è¨€
        file_source_lang = get_source_language_for_file(config.source_language, config.target_language, language_stats)

        # ç¿»è¯‘ä»»åŠ¡åˆ†å‰²
        MAX_LINES = 20  
        total_items = len(items_to_translate)
        num_batches = (total_items + MAX_LINES - 1) // MAX_LINES

        self.info(f" å¼€å§‹å¤„ç†è¡¨æ ¼ç¿»è¯‘ä»»åŠ¡: {os.path.basename(file_path)}")
        self.info(f"    æ€»è®¡ {total_items} è¡Œæ–‡æœ¬, å°†åˆ†ä¸º {num_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ã€‚")
        self.info(f"    å¹¶å‘çº¿ç¨‹æ•°: {max_threads} (ç»“æœå°†åœ¨ä»»åŠ¡å®Œæˆåç»Ÿä¸€åˆ·æ–°)")

        # ç”¨äºæ±‡æ€»æ‰€æœ‰æ‰¹æ¬¡ç»“æœçš„å­—å…¸
        final_updated_items = {}
        # æˆåŠŸ/å¤±è´¥è®¡æ•°
        success_batches = 0
        failed_batches = 0

        # å®šä¹‰å•ä¸ªæ‰¹æ¬¡çš„å·¥ä½œå‡½æ•°
        def translate_worker(batch_idx, batch_items):
            batch_num = batch_idx + 1
            # é‡æ–°è·å–é…ç½®ä»¥æ”¯æŒKeyè½®è¯¢
            current_platform_config = config.get_platform_configuration("translationReq")

            # æ„å»ºå­—å…¸å’Œç´¢å¼•
            source_text_dict = {str(idx): item['source_text'] for idx, item in enumerate(batch_items)}
            index_map = [item['text_index'] for item in batch_items]

            # æ„å»ºæç¤ºè¯
            messages, system_prompt, _ = PromptBuilder.generate_prompt(
                config, source_text_dict, [], file_source_lang
            )
            
            # ç®€å•çš„è¿›åº¦æ—¥å¿—
            print(f" -> [æ‰¹æ¬¡ {batch_num}] æ­£åœ¨å‘é€è¯·æ±‚ ({len(batch_items)}è¡Œ)...")
            
            # å‘é€è¯·æ±‚
            requester = LLMRequester()
            skip, _, response_content, _, _ = requester.sent_request(
                messages, system_prompt, current_platform_config
            )

            if skip:
                print(f" <- [æ‰¹æ¬¡ {batch_num}] âŒ è¯·æ±‚å¤±è´¥")
                return None

            # è§£æå’Œæ ¡éªŒ
            response_dict = ResponseExtractor.text_extraction(self, source_text_dict, response_content)
            check_result, _ = ResponseChecker.check_polish_response_content(
                self, config, response_content, response_dict, source_text_dict
            )
            
            if not check_result:
                print(f" <- [æ‰¹æ¬¡ {batch_num}] âŒ æ ¡éªŒä¸é€šè¿‡")
                return None
            
            # è¿˜åŸåºå·
            restored_response_dict = {
                index_map[int(temp_idx_str)]: text
                for temp_idx_str, text in response_dict.items()
            }

            # ç§»é™¤å‰ç¼€å¹¶è¿”å›
            updated_items = ResponseExtractor.remove_numbered_prefix(self, restored_response_dict)
            print(f" <- [æ‰¹æ¬¡ {batch_num}] âœ… å®Œæˆ (è§£æå‡º {len(updated_items)} æ¡)")
            return updated_items

        # æ‰§è¡Œçº¿ç¨‹æ± 
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            future_to_batch = {}
            # æäº¤ä»»åŠ¡
            for i in range(num_batches):
                start_index = i * MAX_LINES
                end_index = start_index + MAX_LINES
                batch_items = items_to_translate[start_index:end_index]
                
                future = executor.submit(translate_worker, i, batch_items)
                future_to_batch[future] = i

            # å¤„ç†ç»“æœï¼ˆæ­¤å¤„ä»…æ”¶é›†ï¼Œä¸æ›´æ–°UIï¼‰
            for future in as_completed(future_to_batch):
                try:
                    result = future.result()
                    if result:
                        final_updated_items.update(result)
                        success_batches += 1
                    else:
                        failed_batches += 1
                except Exception as e:
                    self.error(f"æ‰¹æ¬¡æ‰§è¡Œå¼‚å¸¸: {e}")
                    failed_batches += 1

        self.info(f" æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæ¯•ã€‚æˆåŠŸ: {success_batches}, å¤±è´¥: {failed_batches}")
        
        # ä»»åŠ¡å…¨éƒ¨å®Œæˆåï¼Œç»Ÿä¸€å‘é€ä¸€æ¬¡UIæ›´æ–°äº‹ä»¶
        if final_updated_items:
            self.info(f" æ­£åœ¨å°† {len(final_updated_items)} æ¡ç¿»è¯‘ç»“æœå†™å…¥è¡¨æ ¼...")
            self.emit(Base.EVENT.TABLE_UPDATE, {
                "file_path": file_path,
                "target_column_index": 2, # ç¿»è¯‘åˆ—
                "updated_items": final_updated_items
            })
        else:
            self.warning(" æœªè·å¾—ä»»ä½•æœ‰æ•ˆç¿»è¯‘ç»“æœï¼Œè¡¨æ ¼æœªæ›´æ–°ã€‚")

        # æ›´æ–°è½¯ä»¶çŠ¶æ€
        Base.work_status = Base.STATUS.IDLE 
        self.info(f" ğŸ³ è¡¨æ ¼ç¿»è¯‘ä»»åŠ¡ç»“æŸ")                         

    # å“åº”è¡¨æ ¼æ¶¦è‰²äº‹ä»¶
    def handle_table_polish_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_table_polish, args=(data,), daemon=True)
        thread.start()

    # è¡¨æ ¼æ–‡æœ¬çš„åˆ†æ‰¹æ¶¦è‰²
    def process_table_polish(self, data: dict):
        """å¤„ç†è¡¨æ ¼æ–‡ä»¶çš„æ‰¹é‡æ¶¦è‰²ä»»åŠ¡"""
        # è§£åŒ…æ•°æ®
        file_path = data.get("file_path")
        items_to_polish = data.get("items_to_polish")

        # å‡†å¤‡é…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.POLISH)
        polishing_mode_selection = config.polishing_mode_selection
        max_threads = config.actual_thread_counts

        # ä»»åŠ¡åˆ†å‰²
        MAX_LINES = 20
        total_items = len(items_to_polish)
        num_batches = (total_items + MAX_LINES - 1) // MAX_LINES

        self.info(f" å¼€å§‹å¤„ç†è¡¨æ ¼æ¶¦è‰²ä»»åŠ¡: {os.path.basename(file_path)}")
        self.info(f"    æ€»è®¡ {total_items} è¡Œæ–‡æœ¬, å°†åˆ†ä¸º {num_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ã€‚")
        self.info(f"    å¹¶å‘çº¿ç¨‹æ•°: {max_threads} (ç»“æœå°†åœ¨ä»»åŠ¡å®Œæˆåç»Ÿä¸€åˆ·æ–°)")

        # ç»“æœæ±‡æ€»å­—å…¸
        final_updated_items = {}
        success_batches = 0
        failed_batches = 0

        # å®šä¹‰å·¥ä½œå‡½æ•°
        def polish_worker(batch_idx, batch_items):
            batch_num = batch_idx + 1
            current_platform_config = config.get_platform_configuration("polishingReq")
            
            source_text_dict = {str(idx): item['source_text'] for idx, item in enumerate(batch_items)}
            translation_text_dict = {str(idx): item['translation_text'] for idx, item in enumerate(batch_items)}
            index_map = [item['text_index'] for item in batch_items]

            messages, system_prompt, _ = PromptBuilderPolishing.generate_prompt(
                config, source_text_dict, translation_text_dict, []
            )
            
            print(f" -> [æ‰¹æ¬¡ {batch_num}] æ­£åœ¨å‘é€è¯·æ±‚ ({len(batch_items)}è¡Œ)...")
            
            requester = LLMRequester()
            skip, _, response_content, _, _ = requester.sent_request(
                messages, system_prompt, current_platform_config
            )

            if skip:
                print(f" <- [æ‰¹æ¬¡ {batch_num}] âŒ è¯·æ±‚å¤±è´¥")
                return None

            # ç¡®å®šæ ¡éªŒåŸºå‡†
            if polishing_mode_selection == "source_text_polish":
                text_dict = source_text_dict
            else:
                text_dict = translation_text_dict

            # è§£ææ ¡éªŒ
            response_dict = ResponseExtractor.text_extraction(self, text_dict, response_content)
            check_result, _ = ResponseChecker.check_polish_response_content(
                self, config, response_content, response_dict, text_dict
            )
            
            if not check_result:
                print(f" <- [æ‰¹æ¬¡ {batch_num}] âŒ æ ¡éªŒä¸é€šè¿‡")
                return None
            
            # è¿˜åŸå’Œæ¸…ç†
            restored_response_dict = {
                index_map[int(temp_idx_str)]: text
                for temp_idx_str, text in response_dict.items()
            }
            updated_items = ResponseExtractor.remove_numbered_prefix(self, restored_response_dict)
            print(f" <- [æ‰¹æ¬¡ {batch_num}] âœ… å®Œæˆ (è§£æå‡º {len(updated_items)} æ¡)")
            return updated_items

        # æ‰§è¡Œçº¿ç¨‹æ± 
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            future_to_batch = {}
            for i in range(num_batches):
                start_index = i * MAX_LINES
                end_index = start_index + MAX_LINES
                batch_items = items_to_polish[start_index:end_index]
                
                future = executor.submit(polish_worker, i, batch_items)
                future_to_batch[future] = i
            
            for future in as_completed(future_to_batch):
                try:
                    result = future.result()
                    if result:
                        final_updated_items.update(result)
                        success_batches += 1
                    else:
                        failed_batches += 1
                except Exception as e:
                    self.error(f"æ‰¹æ¬¡æ‰§è¡Œå¼‚å¸¸: {e}")
                    failed_batches += 1

        self.info(f" æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæ¯•ã€‚æˆåŠŸ: {success_batches}, å¤±è´¥: {failed_batches}")

        # ç»Ÿä¸€å‘é€ UI æ›´æ–°
        if final_updated_items:
            self.info(f" æ­£åœ¨å°† {len(final_updated_items)} æ¡æ¶¦è‰²ç»“æœå†™å…¥è¡¨æ ¼...")
            self.emit(Base.EVENT.TABLE_UPDATE, {
                "file_path": file_path,
                "target_column_index": 3, # æ¶¦è‰²åˆ—
                "updated_items": final_updated_items
            })
        else:
            self.warning(" æœªè·å¾—ä»»ä½•æœ‰æ•ˆæ¶¦è‰²ç»“æœï¼Œè¡¨æ ¼æœªæ›´æ–°ã€‚")

        Base.work_status = Base.STATUS.IDLE 
        self.info(f" ğŸ³ è¡¨æ ¼æ¶¦è‰²ä»»åŠ¡ç»“æŸ")     

    # å“åº”æœ¯è¯­æå–äº‹ä»¶ï¼Œå¹¶å¯åŠ¨æ–°çº¿ç¨‹
    def handle_term_extraction_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_term_extraction, args=(data,), daemon=True)
        thread.start()

    # æœ¯è¯­æå–å¤„ç†æ–¹æ³•
    def process_term_extraction(self, data: dict):
        params = data.get("params", {})
        items_data = data.get("items_data", [])

        if not items_data:
            self.warning("æœ¯è¯­æå–ä»»åŠ¡ä¸­æ­¢ï¼šæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æœ¬ã€‚")
            self.emit(Base.EVENT.TERM_EXTRACTION_DONE, {"results": []})
            return

        self.info(f"å¼€å§‹å¤„ç†æœ¯è¯­æå–ä»»åŠ¡... å‚æ•°: {params}")
        self.info(f"å…±æ”¶åˆ° {len(items_data)} æ¡å¾…å¤„ç†æ•°æ®ã€‚")

        # å®ä¾‹åŒ–ç‹¬ç«‹çš„å¤„ç†å™¨
        processor = NERProcessor()
        
        # è°ƒç”¨å¤„ç†å™¨çš„æ–¹æ³•ï¼Œä¼ å…¥æ­£ç¡®çš„å‚æ•°
        results = processor.extract_terms(
            items_data=items_data,
            model_name=params.get("model_name"), # ä½¿ç”¨ model_name
            entity_types=params.get("entity_types")
        )
        
        self.info(f"æœ¯è¯­æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} ä¸ªæœ¯è¯­ã€‚")

        # å·¥ä½œå®Œæˆåï¼Œå‘å°„å®Œæˆäº‹ä»¶å°†ç»“æœä¼ å›UIçº¿ç¨‹
        self.emit(Base.EVENT.TERM_EXTRACTION_DONE, {"results": results})


    # å“åº”ç¿»è¯‘å¹¶ä¿å­˜æœ¯è¯­è¡¨çš„äº‹ä»¶ï¼Œå¯åŠ¨æ–°çº¿ç¨‹
    def handle_term_translate_save_start(self, event, data: dict):
        thread = threading.Thread(target=self.process_term_translate_and_save, args=(data,), daemon=True)
        thread.start()

    def process_term_translate_and_save(self, data: dict):
        """
        ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æœ¯è¯­çš„ä¸Šä¸‹æ–‡ç¿»è¯‘å’Œä¿å­˜ä»»åŠ¡ã€‚
        """
        # æå–æ•°æ®
        extraction_results = data.get("extraction_results", [])
        if not extraction_results:
            self.warning("æœ¯è¯­ç¿»è¯‘ä»»åŠ¡ä¸­æ­¢ï¼šæœªæ”¶åˆ°ä»»ä½•æå–ç»“æœã€‚")
            self.emit(Base.EVENT.TERM_TRANSLATE_SAVE_DONE, {"status": "no_result", "message": "æœªæ”¶åˆ°æå–ç»“æœ"})
            return

        self.info("â–¶ï¸ å¼€å§‹æ‰§è¡Œã€åŸºäºä¸Šä¸‹æ–‡ç¿»è¯‘å¹¶ä¿å­˜æœ¯è¯­ã€‘ä»»åŠ¡...")

        # æå–æ‰€æœ‰å”¯ä¸€çš„â€œæ‰€åœ¨åŸæ–‡â€(context)
        unique_contexts = sorted(list(set(result['context'] for result in extraction_results)))
        if not unique_contexts:
            self.warning("æœ¯è¯­ç¿»è¯‘ä»»åŠ¡ä¸­æ­¢ï¼šæ²¡æœ‰æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡åŸæ–‡ã€‚")
            self.emit(Base.EVENT.TERM_TRANSLATE_SAVE_DONE, {"status": "no_result", "message": "æ²¡æœ‰æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡"})
            return

        # å‡†å¤‡ç¿»è¯‘é…ç½®
        config = TaskConfig()
        config.initialize()
        config.prepare_for_translation(TaskType.TRANSLATION)
        target_language = config.target_language
        # ä»é…ç½®ä¸­è·å–å®é™…çº¿ç¨‹æ•°
        max_threads = config.actual_thread_counts

        # å°†åŸæ–‡åˆ†æ‰¹å¤„ç†
        MAX_LINES = 20  # æ¯æ‰¹æœ€å¤§åŸæ–‡è¡Œæ•°
        LOG_WIDTH = 50  # æ—¥å¿—æ¡†ç»Ÿä¸€å®½åº¦
        total_items = len(unique_contexts)
        num_batches = (total_items + MAX_LINES - 1) // MAX_LINES

        # æ‰“å°æ•´ä½“ä»»åŠ¡ä¿¡æ¯
        print(f"\nâ•”{'â•' * (LOG_WIDTH-2)}")
        print(f"â•‘{'åŸºäºä¸Šä¸‹æ–‡çš„æœ¯è¯­ç¿»è¯‘ä¸ä¿å­˜'.center(LOG_WIDTH-2)}")
        print(f"â• {'â•' * (LOG_WIDTH-2)}")
        print(f"â”œâ”€ ç‹¬ç«‹ä¸Šä¸‹æ–‡æ€»æ•°: {total_items}")
        print(f"â”œâ”€ å°†åˆ†ä¸º {num_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        print(f"â””â”€ ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ•°: {max_threads}")

        # å®šä¹‰ç”¨äºçº¿ç¨‹æ± çš„å·¥ä½œå‡½æ•°
        def process_batch(batch_contexts, batch_num, total_batches):
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è¯·æ±‚ã€è§£æå’Œè¿”å›ç»“æœ"""
            log_header = f" æ‰¹æ¬¡ {batch_num}/{total_batches} "
            print(f"\nâ•”{'â•' * (LOG_WIDTH-2)}")
            print(f"â•‘{log_header.center(LOG_WIDTH-2)}")
            print(f"â• {'â•' * (LOG_WIDTH-2)}")
            
            user_content = "\n".join(batch_contexts)
            system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æœ¯è¯­æå–ä¸ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œå¹¶æå–å’Œç¿»è¯‘æ–‡æœ¬ä¸­çš„æœ¯è¯­ï¼Œè¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
1.  è¯†åˆ«æœ¯è¯­ï¼šä»æä¾›çš„æ–‡æœ¬ä¸­æå–æ‰€æœ‰å®ä½“åè¯ã€‚ç±»å‹åŒ…æ‹¬ä½†ä¸é™äºï¼šäººåã€åœ°åã€ç»„ç»‡ã€ç‰©å“ã€è£…å¤‡ã€æŠ€èƒ½ã€é­”æ³•ã€ç§æ—ã€ç”Ÿç‰©ç­‰ç­‰ã€‚
2.  ç¿»è¯‘æœ¯è¯­ï¼šå°†æ¯ä¸ªè¯†åˆ«å‡ºçš„æœ¯è¯­å‡†ç¡®ç¿»è¯‘æˆâ€œ{target_language}â€ã€‚
3.  æ ‡æ³¨ç±»å‹ï¼šä¸ºæ¯ä¸ªæœ¯è¯­é™„ä¸Šç®€çŸ­çš„ç±»å‹æ³¨é‡Šï¼ˆä¾‹å¦‚ï¼šâ€œå¥³æ€§äººåâ€ã€â€œåœ°åâ€ã€â€œç»„ç»‡â€ã€â€œç‰©å“â€ï¼‰ã€‚

### è¾“å‡ºæ ¼å¼
ä»¥textareaæ ‡ç­¾æ ¼å¼è¾“å‡ºï¼Œå¦‚:
<textarea>
åŸæ–‡1|è¯‘æ–‡1|æ³¨é‡Š1
åŸæ–‡2|è¯‘æ–‡2|æ³¨é‡Š2
...|...|...
</textarea>
"""
            messages = [{"role": "user", "content": user_content}]

            # æ¯æ¬¡è¯·æ±‚éƒ½è·å–ä¸€æ¬¡é…ç½®ï¼Œä»¥ç¡®ä¿èƒ½è½®è¯¢API Key
            platform_config = config.get_platform_configuration("translationReq")
            
            print(f"â”œâ”€ æ­£åœ¨å‘AIå‘é€è¯·æ±‚ (å…± {len(batch_contexts)} è¡Œ)...\n")
            requester = LLMRequester()
            skip, _, response_content, _, _ = requester.sent_request(
                messages, system_prompt, platform_config
            )

            if skip or not response_content:
                self.error(f"ç¬¬ {batch_num} æ‰¹æ¬¡è¯·æ±‚å¤±è´¥æˆ–è¿”å›å†…å®¹ä¸ºç©ºã€‚")
                print(f"â””â”€ âŒ è¯·æ±‚å¤±è´¥æˆ–æ— å›å¤ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                return [] # è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºå¤±è´¥

            print("â”œâ”€ æ”¶åˆ°å›å¤ï¼Œæ­£åœ¨è§£æ...")
            
            try:
                match = re.search(r'<textarea>(.*?)</textarea>', response_content, re.DOTALL)
                if not match:
                    self.warning(f"ç¬¬ {batch_num} æ‰¹æ¬¡å›å¤ä¸­æœªåŒ¹é…åˆ° <textarea> å—ã€‚")
                    print(f"â””â”€ âš ï¸ å›å¤ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆæœ¯è¯­å—ã€‚")
                    return []

                content_block = match.group(1).strip()
                lines = content_block.split('\n')
                
                batch_parsed_terms = []
                warnings_in_batch = False

                for line in lines:
                    line = line.strip()
                    if not line: continue
                    
                    parts = line.split('|')
                    if len(parts) == 3:
                        src, dst, info = [p.strip() for p in parts]
                        if src:
                            batch_parsed_terms.append({"src": src, "dst": dst, "info": info})
                    else:
                        self.warning(f"è§£æå¤±è´¥ï¼Œæ‰¹æ¬¡ {batch_num} ä¸­æ ¼å¼ä¸ç¬¦: {line}")
                        warnings_in_batch = True
                
                print(f"â”œâ”€ æœ¬æ‰¹æ¬¡æˆåŠŸè§£æ {len(batch_parsed_terms)} æ¡æœ¯è¯­ã€‚")
                if warnings_in_batch:
                    print(f"â””â”€ âš ï¸ æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œä½†æœ‰è§£æè­¦å‘Šã€‚")
                else:
                    print(f"â””â”€ âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚")
                
                return batch_parsed_terms
            except Exception as e:
                self.error(f"è§£æç¬¬ {batch_num} æ‰¹æ¬¡å“åº”æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                print(f"â””â”€ âŒ è§£ææ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                return []

        all_parsed_terms = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = []
            for i in range(num_batches):
                start_index = i * MAX_LINES
                end_index = start_index + MAX_LINES
                batch_contexts = unique_contexts[start_index:end_index]
                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(process_batch, batch_contexts, i + 1, num_batches)
                futures.append(future)
            
            # è·å–å·²å®Œæˆä»»åŠ¡çš„ç»“æœ
            for future in as_completed(futures):
                try:
                    # è·å–å·¥ä½œå‡½æ•°çš„è¿”å›ç»“æœ
                    batch_results = future.result()
                    if batch_results:
                        all_parsed_terms.extend(batch_results)
                except Exception as e:
                    self.error(f"ä¸€ä¸ªæœ¯è¯­ç¿»è¯‘æ‰¹æ¬¡åœ¨æ‰§è¡Œä¸­é‡åˆ°ä¸¥é‡é”™è¯¯: {e}")
        
        # åç»­å¤„ç†é€»è¾‘ä¿æŒä¸å˜
        print("") # åœ¨æ—¥å¿—æœ«å°¾æ·»åŠ ä¸€ä¸ªç©ºè¡Œï¼Œä½¿æ ¼å¼æ›´ç¾è§‚
        self.info("æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨å°†ç»“æœä¿å­˜åˆ°æœ¯è¯­è¡¨...")
        if not all_parsed_terms:
            self.warning("æ‰€æœ‰æ‰¹æ¬¡å‡æœªèƒ½è§£æå‡ºä»»ä½•æœ‰æ•ˆæœ¯è¯­ã€‚ä»»åŠ¡ç»“æŸã€‚")
            self.emit(Base.EVENT.TERM_TRANSLATE_SAVE_DONE, {"status": "no_result", "message": "æœªè§£æåˆ°æœ‰æ•ˆæœ¯è¯­"})
            return

        # åŠ è½½é…ç½®æ–‡ä»¶
        app_config = self.load_config()
        prompt_dictionary_data = app_config.get("prompt_dictionary_data", [])
        
        # è·å–æ—§æœ¯è¯­è¡¨ä¿¡æ¯
        existing_srcs = {item['src'] for item in prompt_dictionary_data}
        
        # å¯¹æ¯”å»é‡
        added_count = 0
        unique_new_terms = {term['src']: term for term in all_parsed_terms}.values()

        for term in unique_new_terms:
            if term['src'] not in existing_srcs:
                prompt_dictionary_data.append(term)
                existing_srcs.add(term['src'])
                added_count += 1
        
        # æ›´æ–°ä¿å­˜æœ¯è¯­è¡¨é…ç½®
        app_config["prompt_dictionary_data"] = prompt_dictionary_data
        self.save_config(app_config)
        
        # æ—¥å¿—è¾“å‡º
        self.info(f"ğŸ³ æœ¯è¯­ç¿»è¯‘ä¸ä¿å­˜ä»»åŠ¡å·²å®Œæˆï¼æˆåŠŸæ·»åŠ  {added_count} ä¸ªæ–°æœ¯è¯­åˆ°æœ¯è¯­è¡¨ã€‚")
        self.emit(Base.EVENT.TERM_TRANSLATE_SAVE_DONE, {
            "status": "success", 
            "message": f"æˆåŠŸæ·»åŠ  {added_count} ä¸ªæ–°æœ¯è¯­ã€‚",
            "added_count": added_count
        })