import json
import os
import re
import time
from ..PluginBase import PluginBase

class TranslationCheckPlugin(PluginBase):
    def __init__(self):
        super().__init__()
        self.name = "TranslationCheckPlugin"
        self.description = "ç¿»è¯‘åŠŸèƒ½æ£€æŸ¥æ’ä»¶ï¼Œç”¨äºç¿»è¯‘ç»“æœä¸åŠŸèƒ½è¿è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬æœ¯è¯­è¡¨ã€ç¦ç¿»è¡¨ã€æ¢è¡Œç¬¦å’Œè‡ªåŠ¨å¤„ç†ç­‰ã€‚\né”™è¯¯ä¿¡æ¯æ–‡ä»¶å°†è¾“å‡ºåˆ° output æ–‡ä»¶å¤¹ã€‚"
        self.visibility = True
        self.default_enable = True
        self.add_event("translation_completed", PluginBase.PRIORITY.LOWEST)

    def load(self):
        pass

    def on_event(self, event_name, config, event_data):
        if event_name == "translation_completed":
            self.check_cache(config, event_data)

    def prepare_regex_patterns(self, exclusion_list_data):
        """å‡†å¤‡æ‰€æœ‰éœ€è¦ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        patterns = []

        # ä»æ­£åˆ™åº“åŠ è½½åŸºç¡€æ­£åˆ™
        with open(os.path.join(".", "Resource", "Regex", "regex.json"), 'r', encoding='utf-8') as f:
            data = json.load(f)
            file_patterns =  [item["regex"] for item in data if isinstance(item, dict) and "regex" in item]
        patterns.extend(file_patterns)

        # åˆå¹¶ç¦ç¿»è¡¨æ•°æ®
        exclusion_patterns = []
        for item in exclusion_list_data:
            if regex := item.get("regex"):
                exclusion_patterns.append(regex)
            elif markers := item.get("markers"): # ä½¿ç”¨ markers å­—æ®µ
                exclusion_patterns.append(re.escape(markers)) # è½¬ä¹‰ markers å¹¶æ·»åŠ 
        patterns.extend(exclusion_patterns)
        return patterns

    def check_cache(self, config, cache_list):
        error_entries = [] # å­˜å‚¨ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯
        output_path = config.label_output_path
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        json_error_filename = f"translation_check_errors_{timestamp}.json" # é”™è¯¯ä¿¡æ¯å•ç‹¬jsonæ–‡ä»¶
        json_error_filepath = os.path.join(output_path, json_error_filename)


        if not os.path.exists(output_path):
            os.makedirs(output_path, exist_ok=True)

        # è·å–é…ç½®é¡¹
        prompt_dictionary_switch = config.prompt_dictionary_switch
        prompt_dictionary_data = config.prompt_dictionary_data
        exclusion_list_switch = config.exclusion_list_switch
        exclusion_list_data = config.exclusion_list_data
        auto_process_text_code_segment = config.auto_process_text_code_segment

        patterns = self.prepare_regex_patterns(exclusion_list_data) if exclusion_list_data else []

        project_report_logged = False # æ ‡è®°é¡¹ç›®æŠ¥å‘Šæ˜¯å¦å·²è¾“å‡º

        total_error_count = 0 # ç»Ÿè®¡æ€»é”™è¯¯æ•°
        check_summary = {
            "prompt_dictionary_errors": 0,
            "exclusion_list_errors": 0,
            "auto_process_errors": 0,
            "newline_errors": 0
        }

        for entry in cache_list:
            project_type = entry.get("project_type","")

            if project_type and not project_report_logged: # é¡¹ç›®è¿è¡Œä¿¡æ¯ï¼Œåªè¾“å‡ºä¸€æ¬¡
                start_time = entry.get("data").get("start_time")
                total_completion_tokens = entry.get("data").get("total_completion_tokens")
                total_requests = entry.get("data").get("total_requests")
                error_requests = entry.get("data").get("error_requests")
                total_line = entry.get("data").get("total_line")
                translated_line = entry.get("data").get("line")
                end_time = time.time()
                elapsed_time = end_time - start_time
                tokens_per_second = total_completion_tokens / elapsed_time if elapsed_time > 0 else 0
                performance_level = self.map_performance_level(tokens_per_second) # ä½¿ç”¨æ–°çš„æ˜ å°„å‡½æ•°

                project_report = [
                    "=" * 60,
                    "          ğŸ’» é¡¹ç›®è¿è¡ŒæŠ¥å‘Š ğŸ’»          ",
                    "=" * 60,
                    f"  ğŸ“Œ é¡¹ç›®ç±»å‹: {project_type}",
                    f"  â± å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}",
                    f"  ğŸ ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}",
                    f"  â³ è¿è¡Œæ—¶é•¿: {elapsed_time:.2f} ç§’",
                    f"  ğŸ“¨ æ€»è¯·æ±‚æ•°: {total_requests}",
                    f"  âŒ é”™è¯¯è¯·æ±‚æ•°: {error_requests}",
                    f"  ğŸ“ æ€»è¡Œæ•°: {total_line}",
                    f"  âœ… ç¿»è¯‘è¡Œæ•°: {translated_line}",
                    f"  âš¡ Tokensé€Ÿåº¦: {tokens_per_second:.2f} tokens/s",
                    "â”€" * 60,
                    "          ğŸ“Š æ€§èƒ½è¯„ä¼°æŠ¥å‘Š ğŸ“Š          ",
                    f"{performance_level}",
                    "=" * 60 + "\n"
                ]
                print("\n".join(project_report)) # é¡¹ç›®æŠ¥å‘Šç›´æ¥è¾“å‡ºåˆ°æ§åˆ¶å°
                project_report_logged = True # æ ‡è®°å·²è¾“å‡º

            elif not project_type: # æ–‡æœ¬æ¡ç›®æ£€æŸ¥
                source_text = entry.get("source_text")
                translated_text = entry.get("translated_text")
                translation_status = entry.get("translation_status")
                storage_path = entry.get("storage_path")
                file_name = entry.get("file_name") if entry.get("file_name") else "Unknown File"
                text_index = entry.get("text_index")

                if translation_status == 7: # å·²è¢«è¿‡æ»¤
                    continue # è·³è¿‡è¢«è¿‡æ»¤çš„æ¡ç›®

                current_entry_errors = [] # å­˜å‚¨å½“å‰æ¡ç›®çš„é”™è¯¯ä¿¡æ¯

                if translation_status == 0: # æœªç¿»è¯‘
                    error_msg = "ğŸš§ [WARNING] æ¡ç›®æœªç¿»è¯‘ " 
                    current_entry_errors.append(error_msg) # è®°å½•é”™è¯¯

                elif translation_status == 1: # å·²ç¿»è¯‘æ¡ç›®
                    # å„é¡¹æ£€æŸ¥ï¼Œå¹¶å°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ° current_entry_errors
                    if prompt_dictionary_switch:
                        errors = self.check_prompt_dictionary(source_text, translated_text, prompt_dictionary_data)
                        if errors:
                            check_summary["prompt_dictionary_errors"] += len(errors)
                            current_entry_errors.extend(errors)
                    if exclusion_list_switch:
                        errors = self.check_exclusion_list(source_text, translated_text, exclusion_list_data)
                        if errors:
                            check_summary["exclusion_list_errors"] += len(errors)
                            current_entry_errors.extend(errors)
                    if auto_process_text_code_segment:
                        errors = self.check_auto_process(source_text, translated_text, patterns)
                        if errors:
                            check_summary["auto_process_errors"] += len(errors)
                            current_entry_errors.extend(errors)
                    errors = self.check_newline(source_text, translated_text)
                    if errors:
                        check_summary["newline_errors"] += len(errors)
                        current_entry_errors.extend(errors)

                if current_entry_errors: # å¦‚æœå½“å‰æ¡ç›®æœ‰é”™è¯¯ï¼Œåˆ™æ·»åŠ åˆ°ç»“æ„åŒ–é”™è¯¯æ—¥å¿—
                    total_error_count += len(current_entry_errors)
                    error_entries.append({
                        "file_name": file_name,
                        "storage_path": storage_path,
                        "text_index": text_index,
                        "source_text": source_text,
                        "translated_text": translated_text,
                        "errors": current_entry_errors
                    })


        # è¾“å‡ºæ£€æŸ¥æ€»ç»“åˆ°æ§åˆ¶å°
        summary_messages = ["\n"+"=" * 60, "          âœ¨ æ£€æŸ¥æ€»ç»“ âœ¨          ", "=" * 60] 
        if total_error_count > 0:
            summary_messages.append(f"âŒ æ€»é”™è¯¯æ¡ç›®æ•°: {total_error_count} âŒ") 
            for check_type, error_count in check_summary.items():
                if error_count > 0:
                    summary_messages.append(f"  - {check_type.replace('_', ' ').title()}: å‘ç° {error_count}ä¸ªé”™è¯¯ âš ï¸") 
        else:
            summary_messages.append("âœ… æ­å–œï¼æ‰€æœ‰æ£€æŸ¥é¡¹å‡æœªå‘ç°é”™è¯¯ ğŸ‰ğŸ‰ğŸ‰") 
        summary_messages.append("=" * 60 + "\n")
        print("\n".join(summary_messages)) # æ§åˆ¶å°è¾“å‡ºæ€»ç»“ä¿¡æ¯


        # å†™å…¥ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯åˆ° JSON æ–‡ä»¶
        if error_entries:
            with open(json_error_filepath, 'w', encoding='utf-8') as json_file:
                json.dump(error_entries, json_file, indent=4, ensure_ascii=False) # ç¼©è¿›å’Œä¸­æ–‡æ”¯æŒ
            print(f"[INFO][TranslationCheckPlugin] ç»“æ„åŒ–é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {json_error_filepath}")

        else:
            print("[INFO][TranslationCheckPlugin] æ²¡æœ‰é”™è¯¯æ¡ç›®ï¼Œæœªç”Ÿæˆç»“æ„åŒ–é”™è¯¯æ—¥å¿—æ–‡ä»¶ã€‚")


    def map_performance_level(self, tokens_per_second):
        """åŸºäºå¯¹æ•°æ­£æ€åˆ†å¸ƒçš„ç™¾åˆ†æ¯”è®¡ç®—æ¨¡å‹ï¼ˆå‡è®¾Î¼=4.0ï¼ŒÏƒ=0.8ï¼‰"""
        from math import log, erf
        # å¯¹æ•°æ­£æ€åˆ†å¸ƒå‚æ•°ï¼ˆé€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°å¯ä»¥æ”¹å˜åˆ†å¸ƒå½¢æ€ï¼‰
        mu, sigma = 4.0, 0.8  # çº¦åˆæ™®é€šç”¨æˆ·åœºæ™¯
        
        def log_normal_cdf(x):
            if x <= 0:
                return 0.0
            return 0.5 * (1 + erf((log(x) - mu) / (sigma * 2**0.5)))
        
        # è®¡ç®—è¶…è¶Šç™¾åˆ†æ¯”ï¼ˆ1 - ç´¯è®¡æ¦‚ç‡ï¼‰
        percentile = (1 - log_normal_cdf(tokens_per_second)) * 100
        
        # æ€§èƒ½ç­‰çº§æ˜ å°„è¡¨ï¼ˆæŒ‰é€Ÿåº¦å‡åºæ’åˆ—ï¼‰
        levels = [
            (20,  "          ğŸŒ èœ—ç‰›é€Ÿ",       "éœ€è¦åŠ æ²¹å“¦", 0.1),
            (50,  "          ğŸš² è‡ªè¡Œè½¦é€Ÿ",     "æ­£å¸¸èµ·æ­¥", 0.3),
            (100, "          ğŸš— æ±½è½¦é€Ÿåº¦",     "æµç•…è¿è¡Œ", 0.6),
            (200, "          ğŸš„ é«˜é“é€Ÿåº¦",     "æ•ˆç‡æƒŠäºº", 0.85),
            (350, "          âœˆï¸ é£æœºé€Ÿåº¦",    "ä¸“ä¸šçº§è¡¨ç°", 0.95),
            (600, "          ğŸš€ ç«ç®­é€Ÿåº¦",    "é¡¶å°–æ°´å¹³", 0.99),
            (1000,"          âš¡ å…‰å­é€Ÿåº¦",     "è¶…è¶Šç‰©ç†æé™", 1.0)
        ]

        # æŸ¥æ‰¾å¯¹åº”çš„ç­‰çº§æè¿°
        for max_speed, name, desc, _ in levels:
            if tokens_per_second <= max_speed:
                break
                
        # è®¡ç®—å®é™…è¶…è¶Šç™¾åˆ†æ¯”ï¼ˆç”¨çº¿æ€§æ’å€¼ä¼˜åŒ–æ˜¾ç¤ºæ•ˆæœï¼‰
        prev_level = next((l for l in reversed(levels) if l[0] < max_speed), None)
        if prev_level:
            ratio = (tokens_per_second - prev_level[0]) / (max_speed - prev_level[0])
            display_percent = min(prev_level[3] + ratio*(percentile/100 - prev_level[3]), 0.999) * 100
        else:
            display_percent = percentile
            
        return f"{name} {desc} \n  ğŸ‰æ­å–œä½ ï¼Œè¶…è¶Šå…¨å®‡å®™ {display_percent:.1f}% çš„ç¿»è¯‘ç”¨æˆ·ï¼ï¼ï¼"


    def check_prompt_dictionary(self, source_text, translated_text, prompt_dictionary_data):
        """æ£€æŸ¥æœ¯è¯­è¡¨åŠŸèƒ½, è¿”å›é”™è¯¯ä¿¡æ¯åˆ—è¡¨"""
        errors = []
        if not prompt_dictionary_data:
            return errors

        for term in prompt_dictionary_data:
            src_term = term.get("src")
            dst_term = term.get("dst")
            if src_term in source_text:
                if dst_term not in translated_text:
                    error_msg = f"ğŸ“š[æœ¯è¯­è¡¨é”™è¯¯] åŸæ–‡ '{src_term}' å­˜åœ¨ï¼Œä½†å¯¹åº”è¯‘æ–‡ç¼ºå°‘æœ¯è¯­ '{dst_term}' " 
                    errors.append(error_msg)
        return errors


    def check_exclusion_list(self, source_text, translated_text, exclusion_list_data):
        """æ£€æŸ¥ç¦ç¿»è¡¨åŠŸèƒ½, è¿”å›é”™è¯¯ä¿¡æ¯åˆ—è¡¨"""
        errors = []
        if not exclusion_list_data:
            return errors

        for item in exclusion_list_data:
            markers = item.get("markers")
            regex = item.get("regex")
            pattern_to_check = regex if regex else re.escape(markers) # ä¼˜å…ˆä½¿ç”¨æ­£åˆ™ï¼Œå¦åˆ™è½¬ä¹‰ markers

            if re.search(pattern_to_check, source_text):
                matches_source = re.findall(pattern_to_check, source_text)
                for match in matches_source: # éå†æ‰€æœ‰åŒ¹é…é¡¹è¿›è¡Œæ£€æŸ¥
                    if match not in translated_text:
                        error_msg = f"ğŸš«[ç¦ç¿»è¡¨é”™è¯¯] æ ‡è®°ç¬¦ '{match}' ï¼Œä½†è¯‘æ–‡ç¼ºå°‘å¯¹åº”å†…å®¹ " 
                        errors.append(error_msg)
        return errors


    def check_auto_process(self, source_text, translated_text, patterns):
        """æ£€æŸ¥è‡ªåŠ¨å¤„ç†åŠŸèƒ½ (åˆå¹¶ç¦ç¿»è¡¨å’Œæ­£åˆ™åº“), è¿”å›é”™è¯¯ä¿¡æ¯åˆ—è¡¨"""
        errors = []
        if not patterns:
            return errors

        for pattern in patterns:
            if re.search(pattern, source_text):
                matches_source = re.findall(pattern, source_text)
                for match in matches_source:
                    if match not in translated_text:
                        error_msg = f"âš™ï¸[è‡ªåŠ¨å¤„ç†é”™è¯¯] æ ‡è®°ç¬¦ '{match}' åŒ¹é…è§„åˆ™ '{pattern}'ï¼Œä½†è¯‘æ–‡ç¼ºå°‘å¯¹åº”å†…å®¹ " 
                        errors.append(error_msg)
        return errors


    def check_newline(self, source_text, translated_text):
        """æ£€æŸ¥æ¢è¡Œç¬¦ä¸€è‡´æ€§, è¿”å›é”™è¯¯ä¿¡æ¯åˆ—è¡¨"""
        errors = []
        source_newlines = source_text.count('\n')
        translated_newlines = translated_text.count('\n')
        if source_newlines != translated_newlines:
            error_msg = f"ğŸ“ƒ[æ¢è¡Œç¬¦é”™è¯¯] åŸæ–‡æœ‰ {source_newlines} ä¸ªæ¢è¡Œç¬¦ï¼Œè¯‘æ–‡æœ‰ {translated_newlines} ä¸ªï¼Œæ•°é‡ä¸ä¸€è‡´ " 
            errors.append(error_msg)
        return errors